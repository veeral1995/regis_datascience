{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This handy piece of code changes Jupyter Notebooks margins to fit your screen.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reddit Comments for a Sentiment Analysis - Walkthough\n",
    "### This tutorial was adapted from a number of sources including: http://www.storybench.org/how-to-scrape-reddit-with-python/ and https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import praw  # Import the Praw library: https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html\n",
    "import pandas as pd  # Import Pandas library: https://pandas.pydata.org/\n",
    "import datetime as dt  # Import datetime library\n",
    "import matplotlib.pyplot as plt  # Import Matplot lib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Praw (Python Reddit API Wrapper) is used to communicate with Reddit\n",
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     user_agent='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will begin viewing the top 100 posts from the 'front page' of '/r/all' within the last month.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('all').top('month', limit = 100)# Define the subreddit of interest. Here we are taking the top 100 posts under 'All' from the past month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the submission title and score:\n",
    "for submission in subreddit:\n",
    "    print(submission.title, submission.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kind of hard to read.  Lets add some more information and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subreddit of interest. Here we are taking the top 100 posts under 'All' from the past month\n",
    "subreddit = reddit.subreddit('all').top('month', limit = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will create a dictionary for a Pandas table.\n",
    "topics_dict = []  # Ceate and empty dictionary\n",
    "topics_dict = { \"title\":[],  # Dictionary headers \n",
    "                \"score\":[], \n",
    "                \"id\":[], \"url\":[], \n",
    "                \"comms_num\": [],\n",
    "                \"created\": [],\n",
    "                \"body\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add elements of each comment to our dictionary\n",
    "for comment in list(subreddit):\n",
    "    topics_dict[\"title\"].append(comment.title)\n",
    "    topics_dict[\"score\"].append(comment.score)\n",
    "    topics_dict[\"id\"].append(comment.id)\n",
    "    topics_dict[\"url\"].append(comment.url)\n",
    "    topics_dict[\"comms_num\"].append(comment.num_comments)\n",
    "    topics_dict[\"created\"].append(comment.created)\n",
    "    topics_dict[\"body\"].append(comment.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a pandas data frame.\n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "topics_data  # Show the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The created column is in unix time. Convert it to normal time.\n",
    "topics_data['created'] = topics_data['created'].astype(int)  # Change the creaded column to an integer.\n",
    "ts = []  # create an empty list for storing timestamps\n",
    "for time in topics_data.index:\n",
    "    ts.append(dt.datetime.fromtimestamp(topics_data['created'][time]))  #  convert unix time to normal time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_data.assign(created=ts)  # assign ts to created column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's better to look at.  We can cearly see the top 100 posts within the last month from the '/r/all' front page. Now we want to pull the top level comments (first comments) found in the top 100 posts within the last month from the '/r/all' front page.  These top level comments these will be used as our baseline sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_subreddit = reddit.subreddit('all').top('month', limit = 100)  # Taking the same top 100 posts under 'All' from the past month\n",
    "comments_all_dict = {\"id\":[],  # Create the empty dictionary\n",
    "    \"comments\":[]}\n",
    "for post in baseline_subreddit:\n",
    "    submission = reddit.submission(id = post)\n",
    "    submission.comments.replace_more(limit=0)  # This line of code expands the comments if “load more comments” and “continue this thread” links are encountered\n",
    "    for top_level_comment in submission.comments: \n",
    "        comments_all_dict[\"id\"].append(top_level_comment.id)  # Saving ID into the dictionary\n",
    "        comments_all_dict[\"comments\"].append(top_level_comment.body)  # Saving comment into the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the comments. They should be in a Pandas Data Table\n",
    "comments_all_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store comments in the the DataFrame\n",
    "comments_base_data = pd.DataFrame.from_dict(comments_all_dict, orient='index').T  # Add and transpose them to data table.\n",
    "comments_base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to prep the comments for a sentment analysis. We tokenize the comments into individual words\n",
    "for word in comments_base_data.comments:  # loop over each word\n",
    "        commentWords = word.split()  # split comments into individual words\n",
    "        for word in commentWords:  # loop over idndividual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will use the sentiment file called AFINN-en-165.txt.  This file contains a sentiment score for 3382 words.  More information can be found here: https://github.com/fnielsen/afinn With the sentiment file we will assign scores to words within the top comments that are found in the AFINN file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentfile = open(\"AFINN-en-165.txt\", \"r\")  # open sentiment file\n",
    "sentiments = {\"-5\": 0, \"-4\": 0, \"-3\": 0, \"-2\": 0, \"-1\": 0, \"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0}  # Create the sentiment dictionary and populate it with zeros \n",
    "scores = {}  # an empty dictionary\n",
    "for line in sentimentfile:  # loop over each word / sentiment score\n",
    "    word, score = line.split(\"\\t\")  # file is tab-delimited\n",
    "    scores[word] = int(score)  # convert the scores to intergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in comments_base_data.comments:  # loop over each word\n",
    "        commentWords = word.split()  # split comments into individual words\n",
    "        for word in commentWords:  # loop over individual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            if word in scores.keys():  # check if word is in sentiment dictionary\n",
    "                score = scores[word]  # check if word is in sentiment dictionary\n",
    "                sentiments[str(score)] += 1  # add one to the sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "print(\"-5 sentiments \", sentiments[\"-5\"])\n",
    "print(\"-4 sentiments \", sentiments[\"-4\"])\n",
    "print(\"-3 sentiments \", sentiments[\"-3\"])\n",
    "print(\"-2 sentiments \", sentiments[\"-2\"])\n",
    "print(\"-1 sentiments \", sentiments[\"-1\"])\n",
    "print(\" 0 sentiments \", sentiments[\"0\"])\n",
    "print(\" 1 sentiments \", sentiments[\"1\"])\n",
    "print(\" 2 sentiments \", sentiments[\"2\"])\n",
    "print(\" 3 sentiments \", sentiments[\"3\"])\n",
    "print(\" 4 sentiments \", sentiments[\"4\"])\n",
    "print(\" 5 sentiments \", sentiments[\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us put the sentiment scores into a dataframe.\n",
    "senti_base = pd.DataFrame(sentiments, index=['Count']).T.reset_index()  # Convert the sentiment dictionary to a data frame, transpose the data, and reset the index\n",
    "senti_base['Value'] = [-5,-4,-3,-2,-1,0,1,2,3,4,5]  # add a score column\n",
    "senti_base = senti_base[['Value', 'Count']]  # Reorder the columns\n",
    "senti_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will plot the data so it is easier to visualize.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.bar(senti_base['Value'], senti_base['Count'], color = 'grey')  # plot x-values, y-values, color\n",
    "plt.xlabel('Sentiment Value')  # add x-label\n",
    "plt.ylabel('Sentiment Count')  # add y-label\n",
    "plt.title('Baseline Reddit Sentiment Analysis')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have baseline comment sentiment data we will build a sentiment file containing sentiment analysis from a specific subreddit.  Below, I picked the /r/aww subreddit, a subreddit for cute and cuddly pictures :-). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_subreddit = reddit.subreddit('aww').top('month', limit = 100)  # pick the subreddit you want to select. Do you want to find top posts, or hot posts?, from what time period?, how many posts?\n",
    "comments_subreddit = {\"id\": [], \"comments\": []}  # Create a dictionary for subreddit comments\n",
    "for post in search_subreddit:\n",
    "    submission = reddit.submission(id=post)\n",
    "    submission.comments.replace_more(limit=0)  # This line of code expands the comments if “load more comments” and “continue this thread” links are encountered\n",
    "    for top_level_comment in submission.comments:\n",
    "        comments_subreddit[\"id\"].append(top_level_comment.id)  # Saving ID into the dictionary\n",
    "        comments_subreddit[\"comments\"].append(top_level_comment.body)  # Saving comment into the dictionary\n",
    "        \n",
    "comments_subreddit_data = pd.DataFrame.from_dict(comments_subreddit, orient='index').T # Create a dataframe for the subreddit comments\n",
    "\n",
    "sentimentfile = open(\"AFINN-en-165.txt\", \"r\")  # open sentiment file\n",
    "scores = {}  # an empty dictionary\n",
    "for line in sentimentfile:  # loop over each word / sentiment score\n",
    "    word, score = line.split(\"\\t\")  # file is tab-delimited\n",
    "    scores[word] = int(score)  # convert the scores to intergers\n",
    "    \n",
    "for word in comments_subreddit_data.comments:  # loop over each word in dataframe\n",
    "         commentWords = word.split()  # split comments into individual words\n",
    "         for word in commentWords:  # loop over idndividual words in each comment\n",
    "            word = word.strip('?:!.,;\"!@()#-')  # remove extraneous characters\n",
    "            word = word.replace(\"\\n\", \"\")  # remove end of line\n",
    "            if word in scores.keys():  # check if word is in sentiment dictionary\n",
    "                score = scores[word]  # check if word is in sentiment dictionary\n",
    "                sentiments[str(score)] += 1  # add one to the score if \n",
    "\n",
    "subreddit_senti = []\n",
    "subreddit_senti = pd.DataFrame(sentiments, index=['Count']).T.reset_index()  # Convert the sentiment dictionary to a data frame, transpose the data, and reset the index\n",
    "subreddit_senti['Value'] = [-5,-4,-3,-2,-1,0,1,2,3,4,5]  # add a score column\n",
    "subreddit_senti = subreddit_senti[['Value', 'Count']]  # Reorder the columns\n",
    "\n",
    "plt.bar(subreddit_senti['Value'], subreddit_senti['Count'], color='blue')  # plot the data x-values, y-values, color\n",
    "plt.xlabel('Sentiment Value')  # add x-label\n",
    "plt.ylabel('Sentiment Count')  # add y-label\n",
    "plt.title('SubReddit Sentiment Analysis')  # add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will overlay the baseline comment sentiment and the subreddit comment sentiment to help compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data together\n",
    "plt.bar(subreddit_senti['Value'] + 0.2,subreddit_senti['Count'], color='blue', label='Sub Reddit') # add subreddit data\n",
    "\n",
    "plt.bar(senti_base['Value'], senti_base['Count'], color='grey', label='Base Reddit') # add baseline data\n",
    "plt.legend() # add the legend\n",
    "\n",
    "plt.xlabel('Sentiment Value')  # add x-label\n",
    "plt.ylabel('Sentiment Count')  # add y-label\n",
    "plt.title('Reddit Sentiment Analysis')  # add title\n",
    "plt.tight_layout()  # tight layout makes it look nice\n",
    "plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this an accurate representation of the data?  What are we missing?  \n",
    "\n",
    "## Let us normalize the data and replot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will add normalized count and normalized scores to the senti_base dataframe.\n",
    "senti_base['Normalized']=senti_base['Count'] / senti_base['Count'].sum()  # Normalize the Count\n",
    "senti_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will add normalized count and normalized scores to the subreddit_senti dataframe.\n",
    "subreddit_senti['Normalized'] = subreddit_senti['Count'] / subreddit_senti['Count'].sum()  # Normalize the Count\n",
    "subreddit_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized data together\n",
    "plt.bar(subreddit_senti['Value'] + 0.2 ,subreddit_senti['Normalized'], color='b', label = 'Sub Reddit')  # add subreddit data\n",
    "\n",
    "plt.bar(senti_base['Value'], senti_base['Normalized'], color='grey', label = 'Base Reddit')  # add baseline data\n",
    "plt.legend()  # add the legend\n",
    "\n",
    "plt.xlabel('Sentiment Value')  # add x-label\n",
    "plt.ylabel('Normalized Count')  # add y-label\n",
    "plt.title('Reddit Sentiment Analysis')  # add title\n",
    "plt.tight_layout()  # tight layout makes it look nice\n",
    "plt.show()  # show the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
